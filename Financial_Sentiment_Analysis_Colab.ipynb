{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a06b8f3",
   "metadata": {},
   "source": [
    "## Cell 1: Install Dependencies & Clone Repository\n",
    "\n",
    "Run this cell first to set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236bcd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers vaderSentiment pandas matplotlib seaborn jupyter kagglehub\n",
    "\n",
    "# Setup environment variables for Colab\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive (optional, for saving outputs)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    print(\"âœ… Google Drive mounted (optional, for persistent storage)\")\n",
    "except:\n",
    "    print(\"âš ï¸  Google Drive not available (running locally or limited access)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ“¥ Setting up SEC filings dataset...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Download SEC EDGAR dataset from Kaggle Hub\n",
    "try:\n",
    "    import kagglehub\n",
    "    \n",
    "    print(\"\\nðŸ”„ Downloading SEC EDGAR Annual Financial Filings (2021)...\")\n",
    "    print(\"   Dataset: pranjalverma08/sec-edgar-annual-financial-filings-2021\")\n",
    "    \n",
    "    # Download dataset\n",
    "    path = kagglehub.dataset_download(\"pranjalverma08/sec-edgar-annual-financial-filings-2021\")\n",
    "    print(f\"âœ… Dataset downloaded to: {path}\")\n",
    "    \n",
    "    # Setup data directory\n",
    "    DATA_BASE_DIR = Path(path)\n",
    "    print(f\"\\nðŸ“‚ Scanning dataset structure...\")\n",
    "    \n",
    "    # List available files\n",
    "    all_files = list(DATA_BASE_DIR.rglob(\"*.json\"))\n",
    "    print(f\"   Found {len(all_files)} JSON files\")\n",
    "    \n",
    "    # Create symlink to local data directory for easier access\n",
    "    LOCAL_DATA_DIR = Path(\"/content/data\")\n",
    "    LOCAL_DATA_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Copy JSON files to local directory (limited to first 50 for demo)\n",
    "    import shutil\n",
    "    sample_files = all_files[:50]\n",
    "    \n",
    "    for idx, file_path in enumerate(sample_files, 1):\n",
    "        try:\n",
    "            dest = LOCAL_DATA_DIR / file_path.name\n",
    "            if not dest.exists():\n",
    "                shutil.copy(str(file_path), str(dest))\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Error copying file {file_path.name}: {e}\")\n",
    "    \n",
    "    print(f\"âœ… Copied {len(list(LOCAL_DATA_DIR.glob('*.json')))} JSON files to /content/data\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âŒ kagglehub not installed, falling back to local data\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error downloading from Kaggle Hub: {e}\")\n",
    "    print(\"   Ensure you have Kaggle credentials configured\")\n",
    "    LOCAL_DATA_DIR = Path(\"/content/data\")\n",
    "    LOCAL_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Set working directory and verify structure\n",
    "os.chdir(\"/content\")\n",
    "print(\"\\nðŸ“‚ Current working directory: /content\")\n",
    "print(f\"âœ… Data directory: /content/data\")\n",
    "\n",
    "# Verify data files\n",
    "data_files = list(LOCAL_DATA_DIR.glob(\"*.json\"))\n",
    "if data_files:\n",
    "    print(f\"\\nâœ… Available data files: {len(data_files)}\")\n",
    "    for i, f in enumerate(data_files[:5], 1):\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"   {i}. {f.name} ({size_kb:.1f} KB)\")\n",
    "    if len(data_files) > 5:\n",
    "        print(f\"   ... and {len(data_files) - 5} more files\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No data files found - dataset may not have downloaded correctly\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b999896",
   "metadata": {},
   "source": [
    "## Cell 2: Setup Environment & Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b8426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure paths for Colab\n",
    "PROJECT_ROOT = Path(\"/content\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"output\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Financial Sentiment Analysis - Google Colab Edition\")\n",
    "print(f\"Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# CORE SENTIMENT ANALYSIS IMPLEMENTATION (from VSCode version)\n",
    "# ============================================================================\n",
    "\n",
    "class FinancialSentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Financial sentiment analyzer using ProsusAI FinBERT model.\n",
    "    \n",
    "    FinBERT is a pre-trained NLP model specifically designed for financial text\n",
    "    sentiment analysis. It provides three-class classification: positive, negative, neutral.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_preference: str = \"auto\", max_length: int = 512):\n",
    "        \"\"\"Initialize the sentiment analyzer with FinBERT or VADER fallback.\"\"\"\n",
    "        self.model_preference = model_preference\n",
    "        self.max_length = max_length\n",
    "        self.use_transformers = False\n",
    "        self.classifier_pipeline = None\n",
    "        self.vader_analyzer = None\n",
    "        \n",
    "        self._setup_models()\n",
    "        print(f\"âœ“ Analyzer initialized: {'FinBERT (Transformers)' if self.use_transformers else 'VADER'}\")\n",
    "    \n",
    "    def _setup_models(self) -> None:\n",
    "        \"\"\"Initialize ML models in order of preference.\"\"\"\n",
    "        if self.model_preference in [\"transformers\", \"auto\"]:\n",
    "            try:\n",
    "                from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "                import torch\n",
    "                \n",
    "                print(\"Loading FinBERT model (ProsusAI/finbert)...\")\n",
    "                \n",
    "                # Determine device\n",
    "                device = 0 if torch.cuda.is_available() else -1\n",
    "                device_name = \"GPU\" if device == 0 else \"CPU\"\n",
    "                print(f\"Using device: {device_name}\")\n",
    "                \n",
    "                # Load tokenizer and model\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "                self.model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "                \n",
    "                # Create pipeline\n",
    "                self.classifier_pipeline = pipeline(\n",
    "                    \"text-classification\",\n",
    "                    model=self.model,\n",
    "                    tokenizer=self.tokenizer,\n",
    "                    device=device,\n",
    "                    return_all_scores=True,\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length,\n",
    "                )\n",
    "                \n",
    "                self.use_transformers = True\n",
    "                print(\"âœ“ FinBERT model loaded successfully\")\n",
    "                return\n",
    "                \n",
    "            except ImportError as e:\n",
    "                print(f\"âš  Transformers library not available: {e}. Falling back to VADER.\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš  Failed to load FinBERT: {e}. Falling back to VADER.\")\n",
    "        \n",
    "        if self.model_preference in [\"vader\", \"auto\"]:\n",
    "            try:\n",
    "                from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "                self.vader_analyzer = SentimentIntensityAnalyzer()\n",
    "                print(\"âœ“ VADER sentiment analyzer loaded\")\n",
    "                return\n",
    "            except ImportError as e:\n",
    "                print(f\"âŒ VADER library not available: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed to load VADER: {e}\")\n",
    "        \n",
    "        raise RuntimeError(\"No sentiment analysis backend available. Install: pip install transformers torch\")\n",
    "    \n",
    "    def split_into_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into sentences using pattern matching.\"\"\"\n",
    "        if not text.strip():\n",
    "            return []\n",
    "        \n",
    "        sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text.strip())\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        sentences = [s for s in sentences if len(s.split()) >= 3]\n",
    "        \n",
    "        return sentences\n",
    "    \n",
    "    def chunk_sentences(self, sentences: List[str], max_chars: int = 800) -> List[str]:\n",
    "        \"\"\"Group sentences into coherent chunks without exceeding character limit.\"\"\"\n",
    "        if not sentences:\n",
    "            return []\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_length = len(sentence)\n",
    "            \n",
    "            if sentence_length > max_chars:\n",
    "                if current_chunk:\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    current_chunk = []\n",
    "                    current_length = 0\n",
    "                \n",
    "                words = sentence.split()\n",
    "                sub_chunk = []\n",
    "                sub_length = 0\n",
    "                \n",
    "                for word in words:\n",
    "                    word_length = len(word) + 1\n",
    "                    if sub_length + word_length > max_chars:\n",
    "                        if sub_chunk:\n",
    "                            chunks.append(\" \".join(sub_chunk))\n",
    "                        sub_chunk = [word]\n",
    "                        sub_length = word_length\n",
    "                    else:\n",
    "                        sub_chunk.append(word)\n",
    "                        sub_length += word_length\n",
    "                \n",
    "                if sub_chunk:\n",
    "                    chunks.append(\" \".join(sub_chunk))\n",
    "                continue\n",
    "            \n",
    "            if current_length + sentence_length + 1 > max_chars and current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [sentence]\n",
    "                current_length = sentence_length\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += sentence_length + 1\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _normalize_finbert_label(self, label: str) -> str:\n",
    "        \"\"\"Normalize FinBERT label to standard format.\"\"\"\n",
    "        label_upper = label.upper()\n",
    "        if \"POSITIVE\" in label_upper or label_upper == \"POS\":\n",
    "            return \"POSITIVE\"\n",
    "        elif \"NEGATIVE\" in label_upper or label_upper == \"NEG\":\n",
    "            return \"NEGATIVE\"\n",
    "        else:\n",
    "            return \"NEUTRAL\"\n",
    "    \n",
    "    def classify_text(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Classify text sentiment using available backend.\"\"\"\n",
    "        if not text.strip():\n",
    "            return {\"label\": \"NEUTRAL\", \"score\": 0.0, \"backend\": \"none\"}\n",
    "        \n",
    "        # Use FinBERT if available\n",
    "        if self.use_transformers and self.classifier_pipeline:\n",
    "            try:\n",
    "                text_to_analyze = text[:2000]\n",
    "                results = self.classifier_pipeline(text_to_analyze)\n",
    "                \n",
    "                if isinstance(results, list) and len(results) > 0:\n",
    "                    if isinstance(results[0], list):\n",
    "                        scores = {item['label']: item['score'] for item in results[0]}\n",
    "                    else:\n",
    "                        scores = {item['label']: item['score'] for item in results}\n",
    "                    \n",
    "                    best_label = max(scores.items(), key=lambda x: x[1])\n",
    "                    normalized_label = self._normalize_finbert_label(best_label[0])\n",
    "                    \n",
    "                    return {\n",
    "                        \"label\": normalized_label,\n",
    "                        \"score\": best_label[1],\n",
    "                        \"backend\": \"finbert\",\n",
    "                        \"all_scores\": scores\n",
    "                    }\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âš  FinBERT analysis failed: {e}. Using VADER.\")\n",
    "        \n",
    "        # Fallback to VADER\n",
    "        if self.vader_analyzer:\n",
    "            vs = self.vader_analyzer.polarity_scores(text)\n",
    "            comp = vs.get(\"compound\", 0.0)\n",
    "            \n",
    "            if comp >= 0.05:\n",
    "                label = \"POSITIVE\"\n",
    "            elif comp <= -0.05:\n",
    "                label = \"NEGATIVE\"\n",
    "            else:\n",
    "                label = \"NEUTRAL\"\n",
    "            \n",
    "            return {\n",
    "                \"label\": label,\n",
    "                \"score\": abs(comp),\n",
    "                \"backend\": \"vader\",\n",
    "                \"vader_scores\": vs\n",
    "            }\n",
    "        \n",
    "        return {\"label\": \"ERROR\", \"score\": 0.0, \"backend\": \"none\"}\n",
    "    \n",
    "    def analyze_text(self, text: str, max_chars: int = 800) -> List[Tuple[int, str, Dict]]:\n",
    "        \"\"\"Comprehensive text analysis with chunking and sentiment classification.\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        paragraphs = [p.strip() for p in re.split(r'\\n{2,}', text) if p.strip()]\n",
    "        if not paragraphs:\n",
    "            paragraphs = [text]\n",
    "        \n",
    "        all_chunks = []\n",
    "        for paragraph in paragraphs:\n",
    "            sentences = self.split_into_sentences(paragraph)\n",
    "            chunks = self.chunk_sentences(sentences, max_chars)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        results = []\n",
    "        for idx, chunk in enumerate(all_chunks, 1):\n",
    "            try:\n",
    "                sentiment_result = self.classify_text(chunk)\n",
    "                results.append((idx, chunk, sentiment_result))\n",
    "            except Exception as e:\n",
    "                results.append((idx, chunk, {\"label\": \"ERROR\", \"score\": 0.0, \"backend\": \"none\"}))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def aggregate_sentiment(self, results: List[Tuple[int, str, Dict]]) -> Dict[str, Any]:\n",
    "        \"\"\"Aggregate sentiment scores from all chunks to determine overall document sentiment.\"\"\"\n",
    "        if not results:\n",
    "            return {\n",
    "                \"label\": \"NEUTRAL\",\n",
    "                \"score\": 0.0,\n",
    "                \"confidence\": 0.0,\n",
    "                \"compound_score\": 0.0,\n",
    "                \"chunk_count\": 0,\n",
    "                \"sentiment_distribution\": {\"Positive\": 0, \"Negative\": 0, \"Neutral\": 0},\n",
    "                \"positive_ratio\": 0.0,\n",
    "                \"negative_ratio\": 0.0,\n",
    "            }\n",
    "        \n",
    "        total_weighted = 0.0\n",
    "        total_score = 0.0\n",
    "        count = 0\n",
    "        sentiment_counts = {\"Positive\": 0, \"Negative\": 0, \"Neutral\": 0, \"ERROR\": 0}\n",
    "        \n",
    "        for _, _, res in results:\n",
    "            label = str(res.get(\"label\", \"\")).upper()\n",
    "            score = float(res.get(\"score\", 0.0) or 0.0)\n",
    "            \n",
    "            if label.startswith(\"POS\"):\n",
    "                sign = 1.0\n",
    "                sentiment_counts[\"Positive\"] += 1\n",
    "            elif label.startswith(\"NEG\"):\n",
    "                sign = -1.0\n",
    "                sentiment_counts[\"Negative\"] += 1\n",
    "            elif label == \"ERROR\":\n",
    "                sign = 0.0\n",
    "                sentiment_counts[\"ERROR\"] += 1\n",
    "            else:\n",
    "                sign = 0.0\n",
    "                sentiment_counts[\"Neutral\"] += 1\n",
    "            \n",
    "            weight = score\n",
    "            total_weighted += sign * weight\n",
    "            total_score += score\n",
    "            count += 1\n",
    "        \n",
    "        if count == 0:\n",
    "            return {\n",
    "                \"label\": \"NEUTRAL\",\n",
    "                \"score\": 0.0,\n",
    "                \"confidence\": 0.0,\n",
    "                \"compound_score\": 0.0,\n",
    "                \"chunk_count\": 0,\n",
    "                \"sentiment_distribution\": sentiment_counts,\n",
    "                \"positive_ratio\": 0.0,\n",
    "                \"negative_ratio\": 0.0,\n",
    "            }\n",
    "        \n",
    "        doc_compound = total_weighted / count\n",
    "        overall_score = total_score / count\n",
    "        \n",
    "        positive_ratio = sentiment_counts[\"Positive\"] / count\n",
    "        negative_ratio = sentiment_counts[\"Negative\"] / count\n",
    "        \n",
    "        if positive_ratio > 0.5:\n",
    "            doc_label = \"Positive\"\n",
    "        elif negative_ratio > 0.5:\n",
    "            doc_label = \"Negative\"\n",
    "        elif doc_compound >= 0.05:\n",
    "            doc_label = \"Positive\"\n",
    "        elif doc_compound <= -0.05:\n",
    "            doc_label = \"Negative\"\n",
    "        elif positive_ratio > negative_ratio and positive_ratio > 0.3:\n",
    "            doc_label = \"Positive\"\n",
    "        elif negative_ratio > positive_ratio and negative_ratio > 0.3:\n",
    "            doc_label = \"Negative\"\n",
    "        else:\n",
    "            doc_label = \"Neutral\"\n",
    "        \n",
    "        confidence = min(overall_score * 1.5, 1.0)\n",
    "        \n",
    "        return {\n",
    "            \"label\": doc_label,\n",
    "            \"score\": abs(doc_compound),\n",
    "            \"confidence\": confidence,\n",
    "            \"compound_score\": doc_compound,\n",
    "            \"chunk_count\": count,\n",
    "            \"sentiment_distribution\": sentiment_counts,\n",
    "            \"positive_ratio\": positive_ratio,\n",
    "            \"negative_ratio\": negative_ratio,\n",
    "        }\n",
    "\n",
    "\n",
    "def load_filing_data(file_path: str, sections: Optional[List[str]] = None) -> Dict[str, str]:\n",
    "    \"\"\"Load and parse SEC filing JSON data.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        if sections is None:\n",
    "            sections_data = {\n",
    "                k: v for k, v in data.items()\n",
    "                if isinstance(v, str) and len(v.strip()) > 100\n",
    "            }\n",
    "        else:\n",
    "            sections_data = {}\n",
    "            for section in sections:\n",
    "                content = data.get(section, \"\")\n",
    "                if content and len(content.strip()) > 100:\n",
    "                    sections_data[section] = content\n",
    "        \n",
    "        return sections_data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ File not found: {file_path}\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"âŒ Invalid JSON in {file_path}: {e}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load filing data: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "print(\"âœ… Environment ready!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb38db",
   "metadata": {},
   "source": [
    "## Cell 3: Load Sample SEC Filing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd87b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and load first available SEC filing\n",
    "available_samples = list(DATA_DIR.glob(\"*.json\"))\n",
    "\n",
    "if available_samples:\n",
    "    SAMPLE_FILE = str(available_samples[0])\n",
    "    print(f\"âœ“ Using sample file: {available_samples[0].name}\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Warning: No sample files found in {DATA_DIR}\")\n",
    "    SAMPLE_FILE = None\n",
    "\n",
    "SECTIONS_OF_INTEREST = ['item_7', 'item_1A']\n",
    "\n",
    "if SAMPLE_FILE:\n",
    "    try:\n",
    "        filing_data = load_filing_data(SAMPLE_FILE, SECTIONS_OF_INTEREST)\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"âœ“ Loaded filing data with {len(filing_data)} sections\")\n",
    "        print(f\"  Available sections: {', '.join(filing_data.keys())}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Display section sizes\n",
    "        for section, text in filing_data.items():\n",
    "            print(f\"  {section}: {len(text):,} characters\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âš ï¸  File not found: {SAMPLE_FILE}\")\n",
    "        filing_data = {}\n",
    "else:\n",
    "    filing_data = {}\n",
    "    print(\"âš ï¸  Could not load sample data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a1f8f",
   "metadata": {},
   "source": [
    "## Cell 4: Initialize FinBERT Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866bec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sentiment analyzer (FinBERT model)\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nðŸ”§ Initializing Sentiment Analyzer...\")\n",
    "print(\"(First run will download FinBERT model ~500MB)\")\n",
    "print()\n",
    "\n",
    "analyzer = FinancialSentimentAnalyzer()\n",
    "\n",
    "print(\"âœ… Ready to analyze filings\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d4c84",
   "metadata": {},
   "source": [
    "## Cell 5: Analyze Sentiment Per Section\n",
    "\n",
    "**Key Question**: Is the tone consistent across sections?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0627b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if filing_data:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nðŸ“Š SENTIMENT COMPARISON - Cross-Section Analysis\")\n",
    "    print(\"=\" * 70)\n",
    "    comparison_data = []\n",
    "    \n",
    "    for section_name, section_text in filing_data.items():\n",
    "        print(f\"\\n  Analyzing {section_name}...\")\n",
    "        section_results = analyzer.analyze_text(section_text)\n",
    "        section_agg = analyzer.aggregate_sentiment(section_results)\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Section': section_name.replace('item_', 'Item ').upper(),\n",
    "            'Sentiment': section_agg['label'],\n",
    "            'Score': f\"{section_agg['score']:.3f}\",\n",
    "            'Confidence': f\"{section_agg['confidence']:.3f}\",\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    # Check for inconsistencies\n",
    "    sentiments = [d['Sentiment'] for d in comparison_data]\n",
    "    if len(set(sentiments)) > 1:\n",
    "        print(f\"âš ï¸  ALERT: Sentiment differs across sections - potential inconsistency detected!\")\n",
    "    else:\n",
    "        print(f\"âœ… Consistent tone: All sections show {sentiments[0]} sentiment\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"âš ï¸  No filing data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4017fefe",
   "metadata": {},
   "source": [
    "## Cell 6: Visualize Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7aa769",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'comparison_df' in locals() and len(comparison_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Convert Score and Confidence to float for plotting\n",
    "    comparison_df['Score_float'] = comparison_df['Score'].astype(float)\n",
    "    comparison_df['Confidence_float'] = comparison_df['Confidence'].astype(float)\n",
    "    \n",
    "    # Sentiment scores\n",
    "    colors = ['#2ecc71' if x > 0 else '#e74c3c' if x < 0 else '#95a5a6' \n",
    "              for x in comparison_df['Score_float']]\n",
    "    axes[0].bar(comparison_df['Section'], comparison_df['Score_float'], color=colors)\n",
    "    axes[0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    axes[0].set_title('Sentiment Score by Section', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Score (Positive â†’ Negative)')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Confidence scores\n",
    "    axes[1].bar(comparison_df['Section'], comparison_df['Confidence_float'], color='#3498db')\n",
    "    axes[1].set_title('Analysis Confidence by Section', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Confidence Score')\n",
    "    axes[1].set_ylim([0, 1.0])\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸  Insufficient data for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b4336c",
   "metadata": {},
   "source": [
    "## Cell 7: Risk Assessment\n",
    "\n",
    "**Key Question**: Are there red flags or risk indicators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a17a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_business_risks(sentiment_results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Assess financial risks from business perspective.\n",
    "    \n",
    "    Returns: Dictionary with risk level, specific risks, and recommendations\n",
    "    \"\"\"\n",
    "    risks = []\n",
    "    severity = \"LOW\"\n",
    "    \n",
    "    positive_ratio = sentiment_results.get('positive_ratio', 0)\n",
    "    negative_ratio = sentiment_results.get('negative_ratio', 0)\n",
    "    confidence = sentiment_results.get('confidence', 0)\n",
    "    \n",
    "    # Risk 1: Over-optimism (potential misleading statements)\n",
    "    if positive_ratio > 0.70:\n",
    "        risks.append(\"ðŸ”´ OVER-OPTIMISM: Unusually positive tone may mask underlying issues\")\n",
    "        severity = \"HIGH\"\n",
    "    \n",
    "    # Risk 2: Distress signals\n",
    "    if negative_ratio > 0.60:\n",
    "        risks.append(\"ðŸ”´ DISTRESS SIGNALS: High negative sentiment may indicate financial problems\")\n",
    "        severity = \"HIGH\"\n",
    "    \n",
    "    # Risk 3: Low confidence (vague/unclear disclosures)\n",
    "    if confidence < 0.30:\n",
    "        risks.append(\"ðŸŸ  VAGUE LANGUAGE: Low confidence suggests unclear or inconsistent disclosures\")\n",
    "        severity = \"MEDIUM\" if severity == \"LOW\" else severity\n",
    "    \n",
    "    # Risk 4: Mixed messaging\n",
    "    if 0.35 < positive_ratio < 0.65 and 0.35 < negative_ratio < 0.65:\n",
    "        risks.append(\"ðŸŸ¡ MIXED SIGNALS: Conflicting narratives - company is hedging statements\")\n",
    "        severity = \"MEDIUM\" if severity == \"LOW\" else severity\n",
    "    \n",
    "    return {\n",
    "        'severity': severity,\n",
    "        'risks': risks if risks else [\"âœ… LOW RISK: Consistent, clear, moderate tone\"],\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "# Analyze all sections for risks\n",
    "if filing_data:\n",
    "    print(\"\\nðŸš¨ RISK ASSESSMENT - Executive Summary\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    all_risks = []\n",
    "    for section_name, section_text in filing_data.items():\n",
    "        section_results = analyzer.analyze_text(section_text)\n",
    "        section_agg = analyzer.aggregate_sentiment(section_results)\n",
    "        section_risks = assess_business_risks(section_agg)\n",
    "        \n",
    "        print(f\"\\nðŸ“Œ {section_name.replace('item_', 'Item ').upper()}\")\n",
    "        print(f\"   Risk Level: {section_risks['severity']} | Confidence: {section_risks['confidence']:.1%}\")\n",
    "        for risk in section_risks['risks']:\n",
    "            print(f\"   {risk}\")\n",
    "        all_risks.append(section_risks)\n",
    "    \n",
    "    # Overall assessment\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ðŸŽ¯ OVERALL ASSESSMENT\")\n",
    "    overall_severity = max([r['severity'] for r in all_risks], key=lambda x: {'LOW': 0, 'MEDIUM': 1, 'HIGH': 2}[x])\n",
    "    print(f\"Risk Level: {overall_severity}\")\n",
    "    \n",
    "    if overall_severity == \"HIGH\":\n",
    "        print(\"âš ï¸  RECOMMENDATION: Conduct thorough audit of disclosure statements\")\n",
    "    elif overall_severity == \"MEDIUM\":\n",
    "        print(\"âš ï¸  RECOMMENDATION: Review specific sections flagged above\")\n",
    "    else:\n",
    "        print(\"âœ… RECOMMENDATION: Standard review procedures sufficient\")\n",
    "else:\n",
    "    print(\"âš ï¸  No filing data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd43b5",
   "metadata": {},
   "source": [
    "## Cell 8: Detailed MD&A Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if filing_data and 'item_7' in filing_data:\n",
    "    print(\"\\nðŸ“‹ DETAILED MD&A ANALYSIS (Management Discussion & Analysis)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    mdna_text = filing_data['item_7']\n",
    "    print(\"\\n  Analyzing MD&A section...\")\n",
    "    mdna_results = analyzer.analyze_text(mdna_text)\n",
    "    mdna_aggregate = analyzer.aggregate_sentiment(mdna_results)\n",
    "    \n",
    "    print(f\"\\nText Length: {len(mdna_text):,} characters\")\n",
    "    print(f\"Chunks Analyzed: {len(mdna_results)}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Sentiment Breakdown:\")\n",
    "    distribution = mdna_aggregate['sentiment_distribution']\n",
    "    total = sum(distribution.values())\n",
    "    for sentiment, count in sorted(distribution.items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / total * 100) if total > 0 else 0\n",
    "        print(f\"   {sentiment:12s}: {count:3d} chunks ({percentage:5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Overall Assessment:\")\n",
    "    print(f\"   Sentiment: {mdna_aggregate['label']}\")\n",
    "    print(f\"   Score: {mdna_aggregate['score']:.3f} (positive=1.0, negative=-1.0)\")\n",
    "    print(f\"   Confidence: {mdna_aggregate['confidence']:.1%}\")\n",
    "    \n",
    "    # Save for downstream analysis\n",
    "    mdna_analysis = {\n",
    "        'results': mdna_results,\n",
    "        'aggregate': mdna_aggregate\n",
    "    }\n",
    "else:\n",
    "    print(\"âš ï¸  MD&A section not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40672386",
   "metadata": {},
   "source": [
    "## Cell 9: Business Actionable Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a08a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“Œ KEY BUSINESS INSIGHTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "insights = [\n",
    "    (\"AUDITORS\", [\n",
    "        \"Use sentiment inconsistency detection to identify sections requiring deeper audit\",\n",
    "        \"Flag unusual optimism in problematic business segments\",\n",
    "        \"Verify claims in sections with vague or conflicting language\"\n",
    "    ]),\n",
    "    (\"INVESTORS\", [\n",
    "        \"Compare sentiment tone to financial metrics (income, cash flow) for coherence\",\n",
    "        \"Watch for over-optimism relative to risk factor disclosures\",\n",
    "        \"Identify companies hedging negative news with cautious language\"\n",
    "    ]),\n",
    "    (\"REGULATORS\", [\n",
    "        \"Monitor for systematically misleading narratives across filings\",\n",
    "        \"Compare sentiment trends year-over-year for consistency\",\n",
    "        \"Cross-check narrative claims against quantitative financial data\"\n",
    "    ])\n",
    "]\n",
    "\n",
    "for stakeholder, use_cases in insights:\n",
    "    print(f\"\\nðŸ‘¥ {stakeholder}\")\n",
    "    for i, case in enumerate(use_cases, 1):\n",
    "        print(f\"   {i}. {case}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab680047",
   "metadata": {},
   "source": [
    "## Cell 10: Batch Processing (Multiple Companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸš€ BATCH PROCESSING - Multiple Companies\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Demo: First show single filing analysis (already done above)\n",
    "print(\"\\nðŸ“Œ SINGLE FILING (Already Analyzed)\")\n",
    "print(\"-\" * 70)\n",
    "if SAMPLE_FILE:\n",
    "    print(f\"File: {SAMPLE_FILE}\")\n",
    "    if 'overall_severity' in locals():\n",
    "        print(f\"Overall Risk Level: {overall_severity}\")\n",
    "    print(\"âœ… Use this when auditing or analyzing a specific company\\n\")\n",
    "\n",
    "# Demo: Now show batch processing capability\n",
    "data_dir = DATA_DIR\n",
    "output_dir = OUTPUT_DIR\n",
    "available_files = list(data_dir.glob(\"*.json\"))\n",
    "\n",
    "if len(available_files) > 1:\n",
    "    print(f\"ðŸ“Œ BATCH PROCESSING ({len(available_files)} companies found)\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\"Processing all filings in data/ directory...\\n\")\n",
    "    \n",
    "    batch_results = []\n",
    "    \n",
    "    for file_idx, file_path in enumerate(available_files[:20], 1):  # Limit to 20 for demo\n",
    "        try:\n",
    "            print(f\"  [{file_idx}/{min(20, len(available_files))}] Processing {file_path.name}...\")\n",
    "            \n",
    "            # Load filing\n",
    "            filing_data_batch = load_filing_data(str(file_path), ['item_7', 'item_1A'])\n",
    "            \n",
    "            if not filing_data_batch:\n",
    "                continue\n",
    "            \n",
    "            # Extract company ID from filename\n",
    "            company_id = file_path.stem.split('_')[0] if '_' in file_path.stem else file_path.stem\n",
    "            \n",
    "            # Analyze each section\n",
    "            for section_name, section_text in filing_data_batch.items():\n",
    "                section_results = analyzer.analyze_text(section_text)\n",
    "                section_agg = analyzer.aggregate_sentiment(section_results)\n",
    "                \n",
    "                batch_results.append({\n",
    "                    'company_id': company_id,\n",
    "                    'file_name': file_path.name,\n",
    "                    'section': section_name,\n",
    "                    'overall_sentiment': section_agg['label'],\n",
    "                    'sentiment_score': section_agg['score'],\n",
    "                    'confidence': section_agg['confidence'],\n",
    "                    'compound_score': section_agg['compound_score'],\n",
    "                    'chunk_count': section_agg['chunk_count'],\n",
    "                    'positive_ratio': section_agg['positive_ratio'],\n",
    "                    'negative_ratio': section_agg['negative_ratio'],\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  âš  Error processing {file_path.name}: {str(e)[:50]}\")\n",
    "            continue\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    if batch_results:\n",
    "        batch_df = pd.DataFrame(batch_results)\n",
    "        \n",
    "        # Save to CSV\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_file = output_dir / f\"batch_analysis_{timestamp}.csv\"\n",
    "        batch_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(\"\\nâœ… Batch Analysis Results:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"\\nProcessed {len(batch_df['company_id'].unique())} companies\")\n",
    "        print(f\"Analyzed {len(batch_df)} section-level entries\")\n",
    "        print(f\"\\nResults saved to: {output_file}\")\n",
    "        \n",
    "        # Display sample results\n",
    "        print(\"\\nSample Results (first 10 entries):\")\n",
    "        display_cols = ['company_id', 'section', 'overall_sentiment', 'sentiment_score', 'confidence']\n",
    "        print(batch_df[display_cols].head(10).to_string(index=False))\n",
    "        \n",
    "        # Generate comparison visualization\n",
    "        if len(batch_df) > 0:\n",
    "            print(\"\\nðŸ“Š Generating batch comparison charts...\")\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            # Chart 1: Sentiment distribution across all companies\n",
    "            sentiment_counts = batch_df['overall_sentiment'].value_counts()\n",
    "            colors = {'Positive': '#2ecc71', 'Negative': '#e74c3c', 'Neutral': '#95a5a6'}\n",
    "            chart_colors = [colors.get(s, '#95a5a6') for s in sentiment_counts.index]\n",
    "            \n",
    "            ax1.bar(sentiment_counts.index, sentiment_counts.values, color=chart_colors)\n",
    "            ax1.set_title('Sentiment Distribution Across All Filings', fontweight='bold')\n",
    "            ax1.set_ylabel('Number of Sections')\n",
    "            ax1.set_xlabel('Sentiment')\n",
    "            \n",
    "            for i, v in enumerate(sentiment_counts.values):\n",
    "                ax1.text(i, v + 0.5, str(v), ha='center', va='bottom')\n",
    "            \n",
    "            # Chart 2: Average sentiment score by company\n",
    "            company_avg = batch_df.groupby('company_id')['sentiment_score'].mean().sort_values(ascending=False)\n",
    "            \n",
    "            bar_colors = ['#2ecc71' if x > 0.3 else '#e74c3c' if x < -0.3 else '#95a5a6' for x in company_avg.values]\n",
    "            ax2.barh(range(len(company_avg)), company_avg.values, color=bar_colors)\n",
    "            ax2.set_yticks(range(len(company_avg)))\n",
    "            ax2.set_yticklabels(company_avg.index)\n",
    "            ax2.set_xlabel('Average Sentiment Score')\n",
    "            ax2.set_title('Average Sentiment by Company', fontweight='bold')\n",
    "            ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"\\nâœ… Use batch processing when:\")\n",
    "        print(\"   - Analyzing industry trends\")\n",
    "        print(\"   - Comparing multiple companies\")\n",
    "        print(\"   - Identifying outliers (unusually optimistic/pessimistic)\")\n",
    "        print(\"   - Generating regulatory reports\")\n",
    "    else:\n",
    "        print(\"âš ï¸  No valid filings found for batch processing\")\n",
    "    \n",
    "else:\n",
    "    print(\"ðŸ“Œ BATCH PROCESSING\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Only {len(available_files)} file(s) available in data/ directory\")\n",
    "    print(\"\\nâœ… Batch processing benefits:\")\n",
    "    print(\"   - Cross-company sentiment comparison\")\n",
    "    print(\"   - Industry trend analysis\")\n",
    "    print(\"   - Risk outlier identification\")\n",
    "    print(\"   - Exportable results (CSV in output/ folder)\")\n",
    "    print(\"\\nðŸ“ˆ Available files:\")\n",
    "    for i, f in enumerate(available_files[:5], 1):\n",
    "        print(f\"   {i}. {f.name}\")\n",
    "    if len(available_files) > 5:\n",
    "        print(f\"   ... and {len(available_files) - 5} more\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c012d8f2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ… **Demo Complete!**\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. **Single Filing Analysis** - Load and analyze a SEC 10-K filing\n",
    "2. **Sentiment Comparison** - Compare tone across sections\n",
    "3. **Risk Assessment** - Flag business risks and inconsistencies\n",
    "4. **Detailed Analysis** - Deep dive into MD&A section\n",
    "5. **Stakeholder Insights** - Business value for Auditors, Investors, Regulators\n",
    "6. **Batch Processing** - Scale analysis to multiple companies\n",
    "\n",
    "### For Your Assignment:\n",
    "- **Deliverable 1 (Prototype)**: This notebook + `sentiment_analysis.py` + `batch_analysis.py`\n",
    "- **Deliverable 2 (Presentation)**: See `Documentations/PRESENTATION_GUIDE.md`\n",
    "\n",
    "### To Share This Demo:\n",
    "1. Save this notebook to your Google Colab\n",
    "2. Click \"Share\" and set permissions\n",
    "3. Copy the link and include in your presentation slides\n",
    "4. During presentation, just click \"Run all\" cells\n",
    "\n",
    "---\n",
    "\n",
    "**HS25 Big Data Assignment - Group Work Part 2**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
